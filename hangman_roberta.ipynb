{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hangman_roberta.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8508ae0be2df4ef4afae649aa68f3828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_435a3cebe4cb43728f6d2a2838d2bf5d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4b3fa5f34eb84cbaadc3f5bf07e3ba20",
              "IPY_MODEL_8c30163f2ad34de8919be1fa8853fedd",
              "IPY_MODEL_c9f3db77f78e43428b420288e6598147"
            ]
          }
        },
        "435a3cebe4cb43728f6d2a2838d2bf5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b3fa5f34eb84cbaadc3f5bf07e3ba20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d2efd691bec44caa96740c9ed0b2c45b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch 0: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c76496a3344495fa803660b1fbd0f64"
          }
        },
        "8c30163f2ad34de8919be1fa8853fedd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_785d42985a354cddb6cfabf777ac09d3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 13496,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 13496,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c0880f250c414e62990c31714a4652da"
          }
        },
        "c9f3db77f78e43428b420288e6598147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7e891db961cc4b76844c16b24ed1a7e3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13496/13496 [29:08&lt;00:00,  7.75it/s, loss=0.218]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a98da298cc2f4902880c3f3e22a3ba14"
          }
        },
        "d2efd691bec44caa96740c9ed0b2c45b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c76496a3344495fa803660b1fbd0f64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "785d42985a354cddb6cfabf777ac09d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c0880f250c414e62990c31714a4652da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7e891db961cc4b76844c16b24ed1a7e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a98da298cc2f4902880c3f3e22a3ba14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9484acbd071a435280b924fb773e185d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b81037144fb14b948c9c31077fffe804",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a4532da27903439787c1f378791eb33d",
              "IPY_MODEL_fe9b602f6b1d4c95a6b98f336e9733e2",
              "IPY_MODEL_6d336c0b233e4347b94b057b1f14bbb9"
            ]
          }
        },
        "b81037144fb14b948c9c31077fffe804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4532da27903439787c1f378791eb33d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8721c64c57394761a699bfe508c4fb83",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch 1: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33d6e711432141808ffc9e90fca5ca89"
          }
        },
        "fe9b602f6b1d4c95a6b98f336e9733e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_677558f1af2b4cbbaf2769974cc7cc40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 13496,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 13496,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_88e1b88af2dc43148c050e57fe63b649"
          }
        },
        "6d336c0b233e4347b94b057b1f14bbb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_489dd281274d435487363709290d6a72",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13496/13496 [29:06&lt;00:00,  7.79it/s, loss=0.227]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0bf6f71c4d4541cfb9460becc8175487"
          }
        },
        "8721c64c57394761a699bfe508c4fb83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33d6e711432141808ffc9e90fca5ca89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "677558f1af2b4cbbaf2769974cc7cc40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "88e1b88af2dc43148c050e57fe63b649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "489dd281274d435487363709290d6a72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0bf6f71c4d4541cfb9460becc8175487": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrcQDQjHKbhW",
        "outputId": "82bf480d-0ac8-42fa-abbf-5cac6b7afc2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 20.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 32.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.16.2\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.2.2)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14774 sha256=1708fab522d021589426e42d8c3a7276eebef729fb8af9e8acc2163922974a21\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u1720rlq/wheels/fb/c3/0e/c4d8ff8bfcb0461afff199471449f642179b74968c15b7a69c\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.2.2\n",
            "    Uninstalling gdown-4.2.2:\n",
            "      Successfully uninstalled gdown-4.2.2\n",
            "Successfully installed gdown-4.4.0\n"
          ]
        }
      ],
      "source": [
        "# Installing required packages\n",
        "!pip install transformers\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the dataset\n",
        "!gdown --id <ID>"
      ],
      "metadata": {
        "id": "NYVzjMLRKlP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required libraries and functions\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from transformers import AdamW\n",
        "from transformers import RobertaTokenizer\n",
        "from transformers import RobertaConfig\n",
        "from transformers import RobertaForMaskedLM\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "O9mog5K0M2TA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "p7WGvoa4M7HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the dataset\n",
        "with open('/content/words_250000_train.txt', 'r', encoding='utf-8') as fp:\n",
        "    lines = fp.read().split('\\n')\n",
        "    lines = lines[:-1]\n",
        "\n",
        "print('Total no. of words :', len(lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYo8AaKhKlNN",
        "outputId": "12a5fc03-f5d5-41be-d5db-c00b824a4a1c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no. of words : 227300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = []\n",
        "for i in lines:\n",
        "  lengths.append(len(i))\n",
        "print('Average Word Length :', sum(lengths)/len(lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXRNcoWCMfav",
        "outputId": "db214fae-0dd6-4dd4-a99a-976ff2ca5ec5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Word Length : 9.347760668719754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mx_len = -1\n",
        "for i in lines:\n",
        "  mx_len = max(mx_len, len(i))\n",
        "print('Maximum Word Length :', mx_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGFvzdo3SQSR",
        "outputId": "b732360c-49e2-415a-d991-e43f2f543aac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Word Length : 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting word lenghts as histogram\n",
        "fig, ax = plt.subplots(figsize =(10, 7))\n",
        "ax.hist(lengths, bins = 30)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "yBk2qiGQMxwl",
        "outputId": "2dc8cc67-6f21-4eea-d65c-d973f758f937"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding most frequent alphabets\n",
        "alph = 'abcdefghijklmnopqrstuvwxyz'\n",
        "d = {}\n",
        "\n",
        "for i in alph:\n",
        "  d[i] = 0\n",
        "\n",
        "for i in lines:\n",
        "  vis = {}\n",
        "  for j in i:\n",
        "    if j not in vis:\n",
        "      d[j] += 1\n",
        "      vis[j] = 1\n",
        "\n",
        "arr = []\n",
        "for i in d:\n",
        "  arr.append((d[i],i))\n",
        "arr.sort(reverse=True)\n",
        "\n",
        "fr_order = ''\n",
        "for i,j in arr:\n",
        "  fr_order += j + ' '\n",
        "\n",
        "print('Alphabets in frequency order :', fr_order)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36tSxrz8OD1P",
        "outputId": "0b1c3722-6537-48fb-ac29-5b041f394008"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alphabets in frequency order : e i a r n o s t l c u d p m h g y b f v k w z x q j \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tp5 = 'eiarn'\n",
        "\n",
        "cnt = {2:0 , 3:0, 4:0, 5:0 }\n",
        "for j in lines:\n",
        "  vis = {}\n",
        "  for i in j:\n",
        "    if i in tp5:\n",
        "      vis[i] = 1\n",
        "  if len(vis)>=2:\n",
        "    cnt[2] += 1\n",
        "  if len(vis)>=3:\n",
        "    cnt[3] += 1\n",
        "  if len(vis)>=4:\n",
        "    cnt[4] += 1\n",
        "  if len(vis) == 5:\n",
        "    cnt[5] += 1\n",
        "\n",
        "print('Percent of words containing atleast 2 letters from top-5 :', cnt[2]/len(lines)*100)\n",
        "print('Percent of words containing atleast 3 letters from top-5 :', cnt[3]/len(lines)*100)\n",
        "print('Percent of words containing atleast 4 letters from top-5 :', cnt[4]/len(lines)*100)\n",
        "print('Percent of words containing all the 5 letters from top-5 :', cnt[5]/len(lines)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW5NgqrKOY1O",
        "outputId": "239fa6d9-ed52-47fe-fa03-62afed4451ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percent of words containing atleast 2 letters from top-5 : 89.04223493180818\n",
            "Percent of words containing atleast 3 letters from top-5 : 64.74615046194457\n",
            "Percent of words containing atleast 4 letters from top-5 : 30.55829300483942\n",
            "Percent of words containing all the 5 letters from top-5 : 7.177738671359436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the RoBERTa model"
      ],
      "metadata": {
        "id": "5CffDhTIPqOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer to map alphabets to vector\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "tokenizer.train(files='words_250000_train.txt', vocab_size=30, special_tokens=['<s>', '<pad>', '</s>', '<mask>'])\n",
        "try:\n",
        "  os.mkdir('./myberto')\n",
        "except:\n",
        "  print('Already Exists!')\n",
        "tokenizer.save_model('myberto')\n",
        "\n",
        "dictionary = {\"<s>\":0,\"<pad>\":1,\"</s>\":2,\"<mask>\":3,\"a\":4,\"b\":5,\"c\":6,\"d\":7,\"e\":8,\"f\":9,\"g\":10,\"h\":11,\"i\":12,\"j\":13,\"k\":14,\"l\":15,\"m\":16,\"n\":17,\"o\":18,\"p\":19,\"q\":20,\"r\":21,\"s\":22,\"t\":23,\"u\":24,\"v\":25,\"w\":26,\"x\":27,\"y\":28,\"z\":29}\n",
        "with open(\"myberto/vocab.json\", \"w\") as outfile:\n",
        "    outfile.write( json.dumps(dictionary) )\n",
        "\n",
        "# initialize the tokenizer using the tokenizer we initialized and saved to file\n",
        "tokenizer = RobertaTokenizer.from_pretrained('myberto')    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdluG3bXPs-L",
        "outputId": "795b9166-57e2-422e-bf8a-01730875e4b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "file myberto/config.json not found\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'RobertaTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the given data into test and train set\n",
        "\n",
        "p = 0.05\n",
        "ind_arr = list(range( len(lines) ))\n",
        "test_inds = random.sample(ind_arr, int(p*len(lines)))\n",
        "\n",
        "test_set = []\n",
        "d = {}\n",
        "for i in test_inds:\n",
        "  test_set.append(lines[i])\n",
        "  d[i] = True\n",
        "\n",
        "train_set = []\n",
        "for i in range(len(lines)):\n",
        "  if i not in d:\n",
        "    train_set.append(lines[i])\n",
        "\n",
        "print('No. of words in Train set :', len(train_set))\n",
        "print('No. of words in Test set :', len(test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb_RiUiRRWmh",
        "outputId": "729cf333-f132-4ab0-9d4a-b267ffe6c3c3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of words in Train set : 215935\n",
            "No. of words in Test set : 11365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing the training data\n",
        "batch = tokenizer(train_set, max_length=mx_len, padding='max_length', truncation=True)\n",
        "\n",
        "labels = torch.tensor([x for x in batch['input_ids']])\n",
        "mask = torch.tensor([x for x in batch['attention_mask']])\n",
        "\n",
        "# make copy of labels tensor, this will be input_ids\n",
        "input_ids = labels.detach().clone()\n",
        "# create random array of floats with equal dims to input_ids\n",
        "rand = torch.rand(input_ids.shape)\n",
        "# mask random 15% where token is not 0 [PAD], 1 [CLS], or 2 [SEP]\n",
        "mask_arr = (rand < .25) * (input_ids != 0) * (input_ids != 1) * (input_ids != 2)\n",
        "# loop through each row in input_ids tensor (cannot do in parallel)\n",
        "for i in range(input_ids.shape[0]):\n",
        "    # get indices of mask positions from mask array\n",
        "    selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
        "    # mask input_ids\n",
        "    nums = {}\n",
        "    for j in selection:\n",
        "      nums[ input_ids[i, j].item() ] = True\n",
        "    for j in range(mx_len):\n",
        "      if input_ids[i, j].item() in nums:\n",
        "        input_ids[i, j] = 3  # our custom [MASK] token == 3"
      ],
      "metadata": {
        "id": "nj7aj7yKRs9p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset as Tensor\n",
        "encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels}\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        # store encodings internally\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of samples\n",
        "        return self.encodings['input_ids'].shape[0]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # return dictionary of input_ids, attention_mask, and labels for index i\n",
        "        return {key: tensor[i] for key, tensor in self.encodings.items()}\n",
        "\n",
        "dataset = Dataset(encodings)\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "KoxXZTXKSxGs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the RoBERTa model with given config\n",
        "config = RobertaConfig(\n",
        "    vocab_size=30,  # we align this to the tokenizer vocab_size\n",
        "    max_position_embeddings=32,\n",
        "    hidden_size=768,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1\n",
        ")\n",
        "model = RobertaForMaskedLM(config)"
      ],
      "metadata": {
        "id": "PsQPRrn7S9eL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using GPU if available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# and move our model over to the selected device\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjhpOgdFTEG0",
        "outputId": "8d4f1482-ecfd-4e92-8544-815d20a44b58"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForMaskedLM(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(30, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(32, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=30, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate training mode\n",
        "model.train()\n",
        "# Initialize optimizer\n",
        "optim = AdamW(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQgeJGnZTGmr",
        "outputId": "03ea6e83-dd5a-4c23-c0a2-2fe9bd244079"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    # setup loop with TQDM and dataloader\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for batch in loop:\n",
        "        # initialize calculated gradients (from prev step)\n",
        "        optim.zero_grad()\n",
        "        # pull all tensor batches required for training\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        # process\n",
        "        # print(input_ids.shape)\n",
        "        # print(attention_mask.shape)\n",
        "        # print(labels.shape)\n",
        "        # print(input_ids[0])\n",
        "        outputs = model(input_ids, attention_mask=attention_mask,labels=labels)\n",
        "        # extract loss\n",
        "        loss = outputs.loss\n",
        "        # calculate loss for every parameter that needs grad update\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optim.step()\n",
        "        # print relevant info to progress bar\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "# Saving the model weights\n",
        "model.save_pretrained('./myberto') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "8508ae0be2df4ef4afae649aa68f3828",
            "435a3cebe4cb43728f6d2a2838d2bf5d",
            "4b3fa5f34eb84cbaadc3f5bf07e3ba20",
            "8c30163f2ad34de8919be1fa8853fedd",
            "c9f3db77f78e43428b420288e6598147",
            "d2efd691bec44caa96740c9ed0b2c45b",
            "6c76496a3344495fa803660b1fbd0f64",
            "785d42985a354cddb6cfabf777ac09d3",
            "c0880f250c414e62990c31714a4652da",
            "7e891db961cc4b76844c16b24ed1a7e3",
            "a98da298cc2f4902880c3f3e22a3ba14",
            "9484acbd071a435280b924fb773e185d",
            "b81037144fb14b948c9c31077fffe804",
            "a4532da27903439787c1f378791eb33d",
            "fe9b602f6b1d4c95a6b98f336e9733e2",
            "6d336c0b233e4347b94b057b1f14bbb9",
            "8721c64c57394761a699bfe508c4fb83",
            "33d6e711432141808ffc9e90fca5ca89",
            "677558f1af2b4cbbaf2769974cc7cc40",
            "88e1b88af2dc43148c050e57fe63b649",
            "489dd281274d435487363709290d6a72",
            "0bf6f71c4d4541cfb9460becc8175487"
          ]
        },
        "id": "VcRdgFvrTPYS",
        "outputId": "32132843-5f28-41c4-b05a-9290b6abe0e1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8508ae0be2df4ef4afae649aa68f3828",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13496 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9484acbd071a435280b924fb773e185d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/13496 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating model"
      ],
      "metadata": {
        "id": "qfo6Kj_xTWhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a pipeline and loading saved model weights\n",
        "fill = pipeline('fill-mask', model='myberto', tokenizer='myberto')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9WnhCJuTZKJ",
        "outputId": "d80b65be-d8a7-4da7-dc2d-ab517acd7633"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the trained model\n",
        "cnt = 0\n",
        "crt_cnt = 0\n",
        "limit = 6\n",
        "special_tokens = ['<s>', '<pad>', '</s>', '<mask>']\n",
        "fr_arr = list('eiarnostlcudpmhgybfvkwzxqj')\n",
        "\n",
        "for word in test_set[1050:1100]:\n",
        "\n",
        "  wrg_gs = 0  \n",
        "  gs = 0\n",
        "  chk = False\n",
        "  used_letters = {}\n",
        "  cnt += 1\n",
        "\n",
        "  while wrg_gs < limit:\n",
        "    \n",
        "    chk = True\n",
        "    tmp = ''\n",
        "    msk_cnt = 0\n",
        "    for i in word:\n",
        "      if i in used_letters:\n",
        "        tmp += i\n",
        "      else:\n",
        "        chk = False\n",
        "        tmp += '<mask>'\n",
        "        msk_cnt += 1\n",
        "    \n",
        "    if chk:\n",
        "      break\n",
        "    \n",
        "    if gs<5:\n",
        "      cur_gs = fr_arr[gs]\n",
        "    else:\n",
        "      pred = fill(tmp, top_k=30)\n",
        "      if msk_cnt == 1:\n",
        "        pred = [ pred ]\n",
        "      arr_g = []\n",
        "      for p in pred:\n",
        "        for i in p:\n",
        "          arr_g.append((i['score'], i['token_str']))\n",
        "      arr_g.sort(reverse = True)\n",
        "      for j,i in arr_g:\n",
        "        if i not in special_tokens and i not in used_letters:\n",
        "          cur_gs = i\n",
        "          break\n",
        "    if cur_gs not in word:\n",
        "      wrg_gs += 1\n",
        "    used_letters[cur_gs] = 1\n",
        "    gs += 1\n",
        "\n",
        "  if chk:\n",
        "    crt_cnt += 1\n",
        "\n",
        "  if cnt%10 == 0:  \n",
        "    print('current win percent :',crt_cnt/cnt)\n",
        "\n",
        "print()\n",
        "print('Final Result')\n",
        "print('---------------------------')\n",
        "print('Total Games won :', crt_cnt)\n",
        "print('Total Games played :', cnt)\n",
        "print('Win percent :', crt_cnt/cnt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUml4lJGTcJ6",
        "outputId": "ab307ddd-ae23-4b0d-fd4f-cc1a2d8ff2a5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current win percent : 0.7\n",
            "current win percent : 0.6\n",
            "current win percent : 0.6333333333333333\n",
            "current win percent : 0.6\n",
            "current win percent : 0.5\n",
            "\n",
            "Final Result\n",
            "---------------------------\n",
            "Total Games won : 25\n",
            "Total Games played : 50\n",
            "Win percent : 0.5\n"
          ]
        }
      ]
    }
  ]
}